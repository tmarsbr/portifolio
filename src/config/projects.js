/**
 * Categorias de Projetos
 * @description Lista de categorias principais para filtros
 */
export const PROJECT_CATEGORIES = [
    'Engenharia de Dados',
    'API & Scraping'
];

/**
 * Subcategorias por Categoria
 * @description Subfiltros espec√≠ficos para cada categoria principal
 */
export const PROJECT_SUBCATEGORIES = {
    'Engenharia de Dados': ['ETL', 'Data Warehouse', 'Orquestra√ß√£o', 'Python', 'dbt', 'Airflow'],
    'API & Scraping': ['Scraping', 'API REST', 'Automa√ß√£o', 'Web Scraping', 'AWS']
};

/**
 * Portf√≥lio de Projetos em Data & Analytics
 * @description Projetos pr√°ticos demonstrando habilidades t√©cnicas
 * @categories "ETL & Data Quality", "Data Warehouse", "Orquestra√ß√£o"
 * @structure Array de objetos com dados completos do projeto
 */
export const projects = [
    // =====================================================
    // PROJETO: Web Scraping Receita Federal (NOVO)
    // =====================================================
    {
        id: 100,
        title: "Pipeline de Web Scraping: Extra√ß√£o de Dados P√∫blicos da Receita Federal",
        impactPhrase: "‚≠ê Destaque | API & Scraping",
        description: "üöÄ Pipeline end-to-end para coleta, tratamento e carga de dados massivos do CNPJ da Receita Federal, utilizando Python para scraping e Apache Spark para processamento distribu√≠do na AWS.",
        longDescription: "Este projeto demonstra a constru√ß√£o de um pipeline de dados robusto para lidar com dados p√∫blicos massivos. Desenvolvi crawlers em Python para automatizar a coleta de arquivos do site da Receita Federal, implementando verifica√ß√µes autom√°ticas de novos arquivos. O processamento dos dados (Empresas, S√≥cios, Estabelecimentos) foi realizado com Apache Spark para garantir escalabilidade. A arquitetura foi desenhada na AWS, utilizando S3 para armazenamento em camadas, Lambda para gatilhos, e EMR para o processamento pesado. O projeto foca em transformar dados brutos e complexos em ativos prontos para an√°lise e consumo em dashboards ou aplica√ß√µes.",
        technologies: ["Python", "Apache Spark", "AWS S3", "AWS Lambda", "Amazon EMR", "Amazon Athena"],
        category: "API & Scraping",
        subcategories: ["Web Scraping", "Python", "AWS"],
        image: `${process.env.PUBLIC_URL}/projects/capa_web_scraping_receita.png`,
        architectureDiagramImage: `${process.env.PUBLIC_URL}/images/arquitetura_web_scraping.png`,
        github: "https://github.com/tmarsbr",
        demo: "",
        metrics: "Processamento distribu√≠do, automa√ß√£o de crawlers, arquitetura cloud AWS",
        featured: true,
        hidden: false,
        complexity: 5,
        date: "2024",
        technicalDifferentiator: "Uso de Apache Spark para processamento de dados massivos da Receita Federal e orquestra√ß√£o em ambiente AWS.",
        architectureDiagram: `graph TD
    subgraph "Origem - Receita Federal"
        A[Dados Abertos CNPJ - FTP/HTTP]
    end

    subgraph "Coleta & Ingest√£o"
        B(Crawlers Python) -- Monitora e Extrai --> A
        B -- Envia para --> C[AWS S3 - Camada Raw]
    end

    subgraph "Processamento Distribu√≠do"
        D(Amazon EMR - Spark) -- L√™ de --> C
        D -- Limpa e Transforma --> E[AWS S3 - Camada Trusted]
        D -- Particiona e Converte --> F[AWS S3 - Camada Refined]
    end

    subgraph "Consumo & An√°lise"
        F -- Consultas via --> G(Amazon Athena)
        G -- Alimenta --> H[Dashboards / BI]
    end

    style B fill:#ff9800,stroke:#333,stroke-width:2px
    style D fill:#e91e63,stroke:#333,stroke-width:2px
    style G fill:#2196f3,stroke:#333,stroke-width:2px`
    },
    // =====================================================
    // PROJETO 1: ETL com Qualidade de Dados (DESTAQUE)
    // =====================================================
    {
        id: 101,
        title: "ETL Robusto: Garantia de Qualidade de Dados com Python e Pydantic",
        impactPhrase: "‚≠ê Destaque | ETL & Data Quality",
        description: "üîç Pipeline ETL para ingest√£o automatizada de dados de vendas de e-commerce (10k+ registros/dia), com valida√ß√µes de qualidade usando Pydantic e testes automatizados com Pytest, garantindo 99.9% de integridade dos dados e reduzindo erros de carga em 80%.",
        longDescription: "Este projeto demonstra dom√≠nio dos fundamentos de ETL com foco em Data Quality. Desenvolvi um pipeline que extrai dados de arquivos CSV/APIs, valida cada registro usando schemas Pydantic (rejeitando dados fora do contrato), aplica transforma√ß√µes de limpeza com Pandas, e carrega em PostgreSQL. O diferencial √© o rigor: cada fun√ß√£o possui testes unit√°rios com Pytest, garantindo que o pipeline funcione de forma consistente. Implementei logging estruturado para rastrear registros rejeitados e relat√≥rios de qualidade. O projeto prova que me preocupo n√£o apenas em mover dados, mas em garantir sua integridade.",
        technologies: ["Python", "Pandas", "Pydantic", "Pytest", "PostgreSQL", "SQL"],
        category: "Engenharia de Dados",
        subcategories: ["ETL", "Python"],
        image: `${process.env.PUBLIC_URL}/projects/capa_etl_qualidade.png`,
        architectureDiagramImage: `${process.env.PUBLIC_URL}/images/arquitetura_etl_qualidade.png`,
        github: "https://github.com/tmarsbr/etl-data-quality",
        demo: "",
        metrics: "Valida√ß√£o de schema, testes automatizados, logging de erros",
        featured: true,
        hidden: false,
        complexity: 3,
        date: "2024",
        technicalDifferentiator: "Foco em Data Quality com Pydantic e testes automatizados com Pytest para garantir a integridade dos dados.",
        architectureDiagram: `graph TD
    subgraph "Origem"
        A[Arquivos CSV ou API P√∫blica]
    end

    subgraph "Pipeline de Ingest√£o e Qualidade"
        B(Script Python) -- Extrai --> A
        B -- Valida com Pydantic --> C{Schema de Dados}
        B -- Limpa e Transforma --> D[Dados V√°lidos]
        B -- Captura Erros --> E[Relat√≥rio de Erros/Logs]
    end

    subgraph "Destino"
        D -- Carrega --> F[Banco de Dados PostgreSQL]
    end

    subgraph "Testes"
        G(Pytest) -- Executa Testes Unit√°rios --> B
    end

    style B fill:#1e90ff,stroke:#333,stroke-width:2px
    style C fill:#ff8c00,stroke:#333,stroke-width:2px
    style G fill:#32cd32,stroke:#333,stroke-width:2px`
    },
    // =====================================================
    // PROJETO 2: Data Warehouse com dbt (DESTAQUE)
    // =====================================================
    {
        id: 102,
        title: "Analytics-Ready Data Warehouse: Modelagem Dimensional com dbt e AWS",
        impactPhrase: "‚≠ê Destaque | Data Warehouse",
        description: "üèõÔ∏è Data Warehouse dimensional para an√°lise de desempenho de campanhas de marketing digital, transformando dados brutos de m√∫ltiplas fontes (Google Ads, Facebook Ads, CRM) armazenados no S3 em modelo Star Schema otimizado com dbt, permitindo an√°lises 5x mais r√°pidas.",
        longDescription: "Projeto de Analytics Engineering que demonstra como preparar dados para an√°lise de forma profissional. Partindo de dados brutos no AWS S3, utilizei Amazon Redshift como Data Warehouse e dbt Core para orquestrar todas as transforma√ß√µes SQL. Implementei a arquitetura em camadas: Staging (dados brutos limpos), Intermediate (transforma√ß√µes de neg√≥cio), e Marts (tabelas Fato e Dimens√£o prontas para BI). O diferencial est√° no uso do dbt: cada modelo SQL √© modular, test√°vel e documentado automaticamente. Implementei testes de integridade (chaves √∫nicas, not null) e gera√ß√£o de documenta√ß√£o com dbt docs, mostrando que sei organizar SQL de forma colaborativa e confi√°vel.",
        technologies: ["dbt", "SQL", "AWS S3", "Amazon Redshift", "Data Modeling", "Star Schema"],
        category: "Engenharia de Dados",
        subcategories: ["Data Warehouse", "dbt"],
        image: `${process.env.PUBLIC_URL}/projects/capa_dw_dbt.png`,
        architectureDiagramImage: `${process.env.PUBLIC_URL}/images/arquitetura_dw_dbt.png`,
        github: "https://github.com/tmarsbr/dbt-analytics-warehouse",
        demo: "",
        metrics: "Modelo dimensional, testes dbt, documenta√ß√£o autom√°tica",
        featured: true,
        hidden: false,
        complexity: 4,
        date: "2024",
        technicalDifferentiator: "Aplica√ß√£o de pr√°ticas de Analytics Engineering com dbt para criar transforma√ß√µes SQL modulares, test√°veis e documentadas.",
        architectureDiagram: `graph TD
    subgraph "Origem - Data Lake"
        A[Dados Brutos no AWS S3]
    end

    subgraph "Data Warehouse & Transforma√ß√£o"
        B(Amazon Redshift) -- Acessa --> A
        C(dbt Core) -- Executa Modelos SQL --> B
        C -- Cria --> D[Camada Staging]
        D -- Transforma --> E[Camada de Produ√ß√£o]
        E -- Tabelas Fato e Dimens√£o --> F{Modelo Dimensional}
    end

    subgraph "Qualidade & Documenta√ß√£o"
        C -- Executa Testes --> G(Testes de dbt)
        C -- Gera Documenta√ß√£o --> H(dbt Docs)
    end

    subgraph "Consumo"
        F -- Acessado por --> I[Ferramenta de BI]
    end

    style C fill:#ff4500,stroke:#333,stroke-width:2px
    style B fill:#2E86C1,stroke:#333,stroke-width:2px`
    },
    // =====================================================
    // PROJETO 3: Pipeline Orquestrado com Airflow (DESTAQUE)
    // =====================================================
    {
        id: 103,
        title: "Pipeline de Dados Automatizado: Orquestra√ß√£o de ETL na AWS com Airflow",
        impactPhrase: "‚≠ê Destaque | Orquestra√ß√£o",
        description: "‚ö° Pipeline automatizado para coleta di√°ria de dados meteorol√≥gicos de APIs p√∫blicas (OpenWeather), orquestrado com Apache Airflow na AWS, processando 50+ localiza√ß√µes com agendamento noturno, retry autom√°tico e alertas de falha via email.",
        longDescription: "Este projeto une ETL e automa√ß√£o em um ambiente de nuvem real. Utilizei Apache Airflow para orquestrar um pipeline completo: extra√ß√£o de dados de API externa, carregamento na camada Raw do S3, transforma√ß√£o com Python/Pandas, e disponibiliza√ß√£o na camada Processed. O Airflow gerencia todo o fluxo atrav√©s de DAGs (Directed Acyclic Graphs) com depend√™ncias claras entre tarefas. Implementei agendamento semanal, retentativas autom√°ticas em caso de falha, e alertas de monitoramento. Todo o ambiente roda em Docker Compose, demonstrando conhecimento em containeriza√ß√£o. O projeto prova que sei pensar em automa√ß√£o e resili√™ncia, preocupa√ß√µes centrais de um Engenheiro de Dados.",
        technologies: ["Apache Airflow", "Python", "AWS S3", "Docker", "Pandas", "Amazon Athena"],
        category: "Engenharia de Dados",
        subcategories: ["Orquestra√ß√£o", "Airflow"],
        image: `${process.env.PUBLIC_URL}/projects/capa_airflow_aws.png`,
        architectureDiagramImage: `${process.env.PUBLIC_URL}/images/arquitetura_airflow_aws.png`,
        github: "https://github.com/tmarsbr/airflow-etl-pipeline",
        demo: "",
        metrics: "DAG orquestrada, agendamento autom√°tico, containeriza√ß√£o Docker",
        featured: true,
        hidden: false,
        complexity: 4,
        date: "2024",
        technicalDifferentiator: "Orquestra√ß√£o com Airflow incluindo agendamento, retentativas e monitoramento, containerizado com Docker.",
        architectureDiagram: `graph TD
    subgraph "Orquestra√ß√£o - Control Plane"
        A(Apache Airflow) -- Agenda e Dispara --> B(DAG)
    end

    subgraph "Execu√ß√£o - Data Plane na AWS"
        B -- Task 1: Extrair --> C{API Externa}
        B -- Task 2: Carregar para Raw --> D[AWS S3 - Camada Raw]
        B -- Task 3: Transformar --> E[Script Python/Pandas]
        E -- L√™ de --> D
        E -- Escreve em --> F[AWS S3 - Camada Processed]
    end

    subgraph "Disponibiliza√ß√£o"
        F -- Acess√≠vel por --> G(Amazon Athena)
    end

    subgraph "Infraestrutura"
        H(Docker) -- Containeriza --> A
    end

    style A fill:#9370DB,stroke:#333,stroke-width:2px
    style H fill:#0db7ed,stroke:#333,stroke-width:2px`
    },
    // =====================================================
    // PROJETOS ANTERIORES (ESCONDIDOS OU SECUND√ÅRIOS)
    // =====================================================
    {
        id: 1,
        title: "An√°lise Explorat√≥ria - Spotify Most Streamed Songs",
        impactPhrase: "‚òÖ Destaque | An√°lise de Dados",
        description: "üéµ Transformei +50k m√∫sicas em insights visuais que revelam padr√µes de sucesso no Spotify usando Python e visualiza√ß√µes com Seaborn.",
        longDescription: "Mergulhei no universo musical para responder uma pergunta intrigante: o que torna uma m√∫sica irresist√≠vel? Usando dataset do Spotify com as faixas mais tocadas globalmente, conduzi uma an√°lise explorat√≥ria completa que revelou insights surpreendentes. Descobri que caracter√≠sticas como 'danceability' e 'energy' t√™m correla√ß√µes espec√≠ficas com o sucesso, mas tamb√©m identifiquei padr√µes temporais que mostram como o gosto musical evolui. O projeto culminou na cria√ß√£o de um 'mapa do sucesso musical' com 8 fatores-chave que podem prever a popularidade de uma m√∫sica.",
        technologies: ["Python", "Pandas", "Matplotlib", "Seaborn", "Jupyter"],
        category: "An√°lise de Dados",
        subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
        image: `${process.env.PUBLIC_URL}/projects/capa_spotify_analysis.png`,
        github: "https://github.com/tmarsbr/data-analyst-project",
        demo: "",
        metrics: "An√°lise de +50k m√∫sicas, identifica√ß√£o de 8 fatores-chave de sucesso",
        featured: false,
        hidden: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 2,
        title: "An√°lise dos Acidentes nas Rodovias Brasileiras",
        impactPhrase: "üéØ Projeto Social | An√°lise de Dados",
        description: "üõ£Ô∏è Analisei +100k registros de acidentes da PRF criando um mapa inteligente de seguran√ßa vi√°ria que identifica pontos cr√≠ticos em 27 estados brasileiros.",
        longDescription: "Este projeto nasceu de uma miss√£o pessoal: usar dados para salvar vidas nas estradas. Analisando registros da Pol√≠cia Rodovi√°ria Federal, criei visualiza√ß√µes interativas que revelam os pontos cr√≠ticos de acidentes em todo territ√≥rio nacional. O mais impactante foi descobrir padr√µes inesperados entre localiza√ß√£o de radares e redu√ß√£o de acidentes, gerando insights que podem influenciar pol√≠ticas p√∫blicas de seguran√ßa. Mapiei 27 estados e identifiquei os hor√°rios, condi√ß√µes clim√°ticas e trechos mais perigosos, criando um verdadeiro 'GPS da seguran√ßa' para as rodovias brasileiras.",
        technologies: ["Python", "Pandas", "Geopandas", "Plotly", "Folium"],
        category: "An√°lise de Dados",
        subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
        image: `${process.env.PUBLIC_URL}/projects/capa_prf_accidents.png`,
        github: "https://github.com/tmarsbr/analise-PRF-",
        demo: "",
        metrics: "An√°lise de +100k acidentes, mapeamento de 27 estados",
        featured: false,
        hidden: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 3,
        title: "Pipeline de Integra√ß√£o - Cl√≠nicas Sanare e Reviver",
        impactPhrase: "‚ö° Projeto Real | Engenharia de Dados",
        description: "üè• Desenvolvi um pipeline ETL robusto que unificou sistemas de duas cl√≠nicas m√©dicas, migrando +10k registros com 99.9% de precis√£o e zero downtime.",
        longDescription: "Enfrentei um desafio real do mundo corporativo: duas cl√≠nicas m√©dicas se fundiram e precisavam unificar seus dados de pacientes, hist√≥ricos e procedimentos. O problema? Sistemas completamente diferentes, formatos incompat√≠veis e zero margem para erros - afinal, eram dados de sa√∫de humana. Desenvolvi uma solu√ß√£o elegante usando programa√ß√£o orientada a objetos, criando um pipeline ETL modular que n√£o apenas integrou os dados, mas tamb√©m implementou valida√ß√µes rigorosas de qualidade. O resultado? Uma migra√ß√£o 100% bem-sucedida que permitiu √† nova empresa operar desde o primeiro dia.",
        technologies: ["Python", "OOP", "ETL", "Data Quality", "Pandas"],
        category: "Engenharia de Dados",
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_integracao_sistemas_medicos.png`,
        github: "https://github.com/tmarsbr/projeto_pipeline",
        demo: "",
        metrics: "Integra√ß√£o de +10k registros, 99.9% de precis√£o na migra√ß√£o",
        featured: false,
        hidden: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 4,
        title: "Extra√ß√£o e An√°lise - Reposit√≥rios GitHub",
        impactPhrase: "üî• Automatiza√ß√£o | API & Scraping",
        description: "üêô Sistema automatizado que extraiu e analisou dados de +1000 reposit√≥rios de 15 grandes empresas tech via API GitHub, revelando tend√™ncias de desenvolvimento.",
        longDescription: "Queria entender como as big techs constroem software em escala. O desafio? Coletar dados de milhares de reposit√≥rios sem violar rate limits da API. Desenvolvi um sistema robusto com retry autom√°tico, cache inteligente e extra√ß√£o otimizada que processou +1000 repos de 15 empresas (Netflix, Uber, Airbnb...). O resultado revelou padr√µes surpreendentes: quais linguagens dominam, como evolu√≠ram nos √∫ltimos anos e o que isso diz sobre o futuro do desenvolvimento.",
        technologies: ["Python", "GitHub API", "Pandas", "Requests", "JSON"],
        category: "API & Scraping",
        subcategories: ["Scraping", "API REST"],
        image: `${process.env.PUBLIC_URL}/projects/capa_github_analysis.png`,
        github: "https://github.com/tmarsbr/Projeto_api",
        demo: "",
        metrics: "An√°lise de +1000 reposit√≥rios, 15 empresas tech",
        featured: false,
        hidden: true,
        complexity: 3,
        date: "2024"
    },
    {
        id: 5,
        title: "Pipeline Python - MongoDB - MySQL",
        impactPhrase: "üöÄ Integra√ß√£o | Engenharia de Dados",
        description: "üîÑ Pipeline completo para e-commerce integrando MongoDB e MySQL, reduzindo em 70% o tempo de an√°lise da equipe de BI com processamento automatizado.",
        longDescription: "A equipe de BI de um e-commerce perdia horas toda semana fazendo joins manuais entre MongoDB e MySQL. Constru√≠ um pipeline ETL que automatizou todo o processo: extra√ß√£o de dados n√£o-estruturados do Mongo, transforma√ß√£o com valida√ß√µes de qualidade, e carga otimizada no MySQL. O resultado? 70% menos tempo gasto em prepara√ß√£o de dados e uma base unificada pronta para an√°lise. Eng. de dados resolvendo problemas reais de neg√≥cio.",
        technologies: ["Python", "MongoDB", "MySQL", "ETL", "PyMongo"],
        category: "Engenharia de Dados",
        subcategories: ["ETL/ELT", "SQL"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_mongo_mysql.png`,
        github: "https://github.com/tmarsbr/pipeline-python-mongo-mysql",
        demo: "",
        metrics: "Redu√ß√£o de 70% no tempo de an√°lise da equipe de BI",
        featured: false,
        hidden: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 6,
        title: "An√°lise de Cr√©dito com Machine Learning",
        impactPhrase: "üõ†Ô∏è Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üí≥ Modelo de score de cr√©dito com Machine Learning em fase de testes.",
        longDescription: "Projeto de an√°lise de cr√©dito utilizando t√©cnicas de machine learning para avalia√ß√£o de risco. Em desenvolvimento com foco em algoritmos de classifica√ß√£o e an√°lise de padr√µes de inadimpl√™ncia.",
        technologies: ["Python", "Scikit-learn", "Pandas", "Machine Learning"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_credito_ml.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 7,
        title: "People Analytics - Insights de RH",
        impactPhrase: "üöß Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üë• Sistema de an√°lise de dados de RH para insights estrat√©gicos em gest√£o de pessoas.",
        longDescription: "Projeto focado na aplica√ß√£o de People Analytics para tomada de decis√£o em gest√£o de pessoas, incluindo an√°lise de turnover, performance e engajamento de colaboradores.",
        technologies: ["Python", "Pandas", "Plotly", "Statistics"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_people_analytics.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 8,
        title: "Previs√£o de Demandas - S√©ries Temporais",
        impactPhrase: "‚è±Ô∏è Em Constru√ß√£o | Ci√™ncia de Dados",
        description: "üìà Modelos de previs√£o de demanda utilizando algoritmos de s√©ries temporais.",
        longDescription: "Projeto focado em previs√£o de demandas utilizando algoritmos de s√©rie temporal avan√ßados, incluindo ARIMA, Prophet e redes neurais para forecasting empresarial.",
        technologies: ["Python", "Prophet", "ARIMA", "TensorFlow"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_previsao_demandas.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 9,
        title: "Sistema Antifraude com IA",
        impactPhrase: "üîí Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üõ°Ô∏è Sistema de detec√ß√£o de fraudes com m√©todos estat√≠sticos e machine learning.",
        longDescription: "Modelo de escore antifraude utilizando t√©cnicas avan√ßadas de machine learning para detectar padr√µes suspeitos e prevenir fraudes em transa√ß√µes financeiras.",
        technologies: ["Python", "Scikit-learn", "Anomaly Detection", "Deep Learning"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_fraude_financeira.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 10,
        title: "Automatizando Infraestrutura de Processamento de Dados com AWS EMR e Apache Flink",
        impactPhrase: "‚òÅÔ∏è Cloud Infrastructure | Engenharia de Dados",
        description: "‚ö° Infraestrutura como c√≥digo para processamento de big data em tempo real utilizando AWS EMR, Apache Flink e Terraform para escalabilidade autom√°tica.",
        longDescription: "Projeto focado na automa√ß√£o completa de infraestrutura de processamento de dados em nuvem. Utilizando AWS EMR (Elastic MapReduce) para clusters gerenciados e Apache Flink para processamento de streams em tempo real, toda a infraestrutura √© provisionada via Terraform seguindo pr√°ticas de IaC (Infrastructure as Code). O sistema inclui auto-scaling, monitoramento integrado e otimiza√ß√£o de custos, demonstrando como construir pipelines de dados robustos e escal√°veis na AWS.",
        technologies: ["AWS EMR", "Apache Flink", "Terraform", "IaC", "Python"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["Cloud AWS", "IaC"],
        image: `${process.env.PUBLIC_URL}/projects/capa_aws_emr_flink.png`,
        github: "https://github.com/tmarsbr/aws-emr-flink-portfolio",
        demo: "",
        metrics: "Infraestrutura 100% automatizada, processamento em tempo real",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 11,
        title: "Pipeline PySpark Para Extrair, Transformar e Carregar Arquivos JSON em Banco de Dados",
        impactPhrase: "üî• Big Data Processing | Engenharia de Dados",
        description: "üöÄ Pipeline robusto de ETL desenvolvido com PySpark para processar grandes volumes de dados JSON, aplicando transforma√ß√µes complexas e carregamento otimizado com processamento distribu√≠do.",
        longDescription: "Imagine uma empresa que coleta dados de APIs, logs de aplica√ß√µes ou sensores IoT, todos em formato JSON. Esses dados precisam ser extra√≠dos, limpos, transformados e carregados em um banco de dados relacional ou NoSQL para an√°lises posteriores. Como engenheiro de dados, meu desafio era criar um pipeline escal√°vel que pudesse processar grandes volumes de JSONs, garantindo integridade, performance e facilidade de manuten√ß√£o.",
        technologies: ["PySpark", "Apache Spark", "JSON", "ETL", "SQL", "Processamento Distribu√≠do"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT", "PySpark", "DataOps", "Docker", "SQL/NoSQL"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_etl_json.png`,
        github: "https://github.com/tmarsbr/pipeline-pyspark-etl-json",
        demo: "",
        metrics: "Processamento distribu√≠do, transforma√ß√µes complexas, escalabilidade horizontal",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 12,
        title: "Pipeline de Limpeza e Transforma√ß√£o Para Aplica√ß√µes de IA com PySpark SQL",
        impactPhrase: "ü§ñ AI Data Preparation | Engenharia de Dados",
        description: "‚ú® Pipeline especializado em prepara√ß√£o de dados para modelos de IA usando PySpark SQL, garantindo qualidade e consist√™ncia dos datasets de treinamento.",
        longDescription: "Pipeline avan√ßado de prepara√ß√£o de dados especificamente desenhado para alimentar aplica√ß√µes de Intelig√™ncia Artificial. Utilizando PySpark SQL para opera√ß√µes eficientes, o sistema implementa t√©cnicas sofisticadas de limpeza, detec√ß√£o de anomalias, feature engineering e normaliza√ß√£o. Inclui valida√ß√µes automatizadas de qualidade, tratamento inteligente de valores ausentes e gera√ß√£o de estat√≠sticas descritivas para garantir que os dados estejam prontos para treinamento de modelos de ML.",
        technologies: ["PySpark", "Spark SQL", "Feature Engineering", "Data Quality"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_ai_pipeline.png`,
        github: "https://github.com/tmarsbr/pyspark-ai-data-pipeline",
        demo: "",
        metrics: "Prepara√ß√£o de dados para IA, valida√ß√µes automatizadas",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 13,
        title: "Automa√ß√£o de Testes de Modelos de Dados no dbt",
        impactPhrase: "üß™ Data Testing | Engenharia de Dados",
        description: "üîç Framework completo de testes automatizados para modelos de dados usando dbt, garantindo qualidade e confiabilidade dos pipelines anal√≠ticos.",
        longDescription: "Implementa√ß√£o de um framework robusto de testes automatizados para modelos de dados utilizando dbt (data build tool). O sistema inclui testes de integridade referencial, valida√ß√µes de qualidade de dados, testes de regress√£o e monitoramento cont√≠nuo. Desenvolvido com foco em DataOps, o projeto demonstra como implementar CI/CD para dados, incluindo testes unit√°rios para transforma√ß√µes SQL, valida√ß√µes de schema e alertas autom√°ticos para anomalias nos dados.",
        technologies: ["dbt", "SQL", "Data Testing", "DataOps", "CI/CD"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["DataOps"],
        image: `${process.env.PUBLIC_URL}/projects/capa_dbt_automated_testing.png`,
        github: "https://github.com/tmarsbr/dbt-automated-testing",
        demo: "",
        metrics: "Framework de testes automatizados, qualidade de dados garantida",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 14,
        title: "Movimenta√ß√£o de Dados Entre Bancos de Dados com Airbyte",
        impactPhrase: "üîÑ Data Integration | Engenharia de Dados",
        description: "üåê Solu√ß√£o de integra√ß√£o de dados usando Airbyte para sincroniza√ß√£o autom√°tica entre diferentes fontes de dados, garantindo consist√™ncia e atualiza√ß√£o em tempo real.",
        longDescription: "Implementa√ß√£o de uma solu√ß√£o completa de integra√ß√£o de dados utilizando Airbyte para orquestrar a movimenta√ß√£o entre diferentes sistemas de banco de dados. O projeto demonstra como configurar conectores personalizados, implementar transforma√ß√µes durante a sincroniza√ß√£o e garantir a consist√™ncia dos dados entre ambientes. Inclui monitoramento de performance, tratamento de falhas e estrat√©gias de recupera√ß√£o, mostrando como construir pipelines de dados resilientes e escal√°veis.",
        technologies: ["Airbyte", "PostgreSQL", "MySQL", "Data Integration", "ETL"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_automacao_etl.png`,
        github: "https://github.com/tmarsbr/airbyte-etl-portfolio",
        demo: "",
        metrics: "Sincroniza√ß√£o autom√°tica entre DBs, integra√ß√£o de dados resiliente",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 15,
        title: "Pipeline de Dados Clim√°ticos ‚Äì Airflow",
        impactPhrase: "üå§Ô∏è Orquestra√ß√£o | Engenharia de Dados",
        description: "‚ö° Pipeline automatizado com Apache Airflow que extrai dados meteorol√≥gicos da API Visual Crossing Weather, processa e estrutura datasets semanalmente para planejamento tur√≠stico em Boston.",
        longDescription: "Desenvolvimento de um pipeline robusto de dados clim√°ticos utilizando Apache Airflow para uma empresa de turismo em Boston. O sistema resolve o desafio de coletar e processar dados meteorol√≥gicos de forma consistente e automatizada, permitindo planejamento inteligente de roteiros tur√≠sticos baseados em condi√ß√µes clim√°ticas. Implementa DAGs (Directed Acyclic Graphs) que extraem dados da API Visual Crossing Weather, processam informa√ß√µes meteorol√≥gicas e armazenam datasets organizados por semana. O pipeline inclui separa√ß√£o especializada de dados (temperaturas, condi√ß√µes clim√°ticas) e execu√ß√£o semanal automatizada, capacitando decis√µes data-driven que melhoram a experi√™ncia do cliente e otimizam opera√ß√µes tur√≠sticas.",
        technologies: ["Apache Airflow", "Python", "API Integration", "ETL", "pandas"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["DataOps", "ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_climatico_airflow.png`,
        github: "https://github.com/tmarsbr/airflowalura",
        demo: "",
        metrics: "Pipeline semanal automatizado, extra√ß√£o de dados meteorol√≥gicos estruturados",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 16,
        title: "Pipeline ETL Distribu√≠do com Apache Airflow e AWS EMR",
        impactPhrase: "üöÄ Enterprise-Ready | Big Data Engineering",
        description: "‚ö° Pipeline completo de ETL processando 5.8M registros de voos com Apache Airflow e AWS EMR, demonstrando arquitetura enterprise para processamento distribu√≠do em escala.",
        longDescription: "Projeto completo de Data Engineering de n√≠vel profissional que implementa um pipeline ETL robusto para processamento de big data. O sistema processa 5,819,079 registros de voos (564.96 MB) convertendo dados de CSV para formato Parquet otimizado, utilizando Apache Airflow 2.8.2 para orquestra√ß√£o e AWS EMR 6.15.0 com Spark 3.4.1 para processamento distribu√≠do. Inclui containeriza√ß√£o com Docker Compose, storage otimizado no S3 com particionamento por ano/m√™s/dia, e configura√ß√£o completa de VPC + IAM para seguran√ßa enterprise. O projeto demonstra resolu√ß√£o de desafios t√©cnicos reais incluindo compatibilidade de inst√¢ncias AWS (m5‚Üím4), configura√ß√£o VPC obrigat√≥ria, permiss√µes IAM corretas, capacidade de zona e corre√ß√£o de tipos de dados no Spark. Documenta√ß√£o completa para diferentes audi√™ncias (t√©cnica e executiva) e pr√°ticas de produ√ß√£o com monitoramento, logs detalhados e auto-termina√ß√£o para otimiza√ß√£o de custos.",
        technologies: ["Apache Airflow", "AWS EMR", "Apache Spark", "Docker", "S3", "Parquet", "VPC"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["Cloud AWS", "DataOps", "ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_airflow_emr_pipeline.png`,
        github: "https://github.com/tmarsbr/apache-airflow-emr-pipeline",
        demo: "",
        metrics: "5.8M registros processados, 564.96 MB otimizados, pipeline enterprise-ready",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 17,
        title: "Constru√ß√£o de um Datalake e Lakehouse do Zero ‚Äì AWS & Databricks",
        impactPhrase: "üèóÔ∏è Data Lakehouse | Engenharia de Dados",
        description: "üèõÔ∏è Arquitetura moderna de dados implementando Datalake e Lakehouse do zero com AWS e Databricks, organizando dados em camadas RAW ‚Üí BRONZE ‚Üí SILVER ‚Üí GOLD.",
        longDescription: "Projeto completo de constru√ß√£o de uma arquitetura de dados moderna implementando conceitos de Datalake e Lakehouse utilizando AWS e Databricks. A solu√ß√£o organiza dados em camadas (RAW ‚Üí BRONZE ‚Üí SILVER ‚Üí GOLD) com CDC (Change Data Capture) e CDF (Change Data Feed) para ingest√£o e transforma√ß√£o cont√≠nua. Inclui processamento em tempo real, governan√ßa de dados, e cria√ß√£o de cubos anal√≠ticos na camada GOLD para alimentar dashboards e an√°lises de neg√≥cio. Demonstra dom√≠nio em arquiteturas modernas de dados com foco em escalabilidade, performance e governan√ßa.",
        technologies: ["AWS", "Databricks", "Delta Lake", "CDC", "CDF", "Spark", "Data Lakehouse"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["Cloud AWS", "Databricks", "ETL/ELT", "Streaming", "Data Lakehouse", "DataOps", "Dashboard"],
        image: `${process.env.PUBLIC_URL}/projects/capa_neon_data_lakehouse.png`,
        github: "",
        demo: "",
        metrics: "Arquitetura Lakehouse completa, processamento em tempo real com CDC/CDF",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 18,
        title: "Pipeline CDC - Ingest√£o Automatizada Kaggle ‚Üí AWS S3",
        impactPhrase: "üîÑ Change Data Capture | Engenharia de Dados",
        description: "üìä Sistema de ETL automatizado com CDC que detecta mudan√ßas em datasets do Kaggle, gerando arquivos Parquet otimizados para alimentar Data Lake na AWS S3.",
        longDescription: "Pipeline de ingest√£o incremental (Parte 1/2 de arquitetura completa de Data Lake) que automatiza a extra√ß√£o de dados do Kaggle com Change Data Capture (CDC). O sistema detecta e captura automaticamente tr√™s tipos de opera√ß√µes: INSERT (novos registros), UPDATE (altera√ß√µes em registros existentes) e DELETE (registros removidos), gerando arquivos Parquet com compress√£o Snappy e metadados CDC estruturados. Implementa compara√ß√£o inteligente de snapshots (anterior vs atual) para identificar mudan√ßas, evitando reprocessamento completo de datasets. Utiliza Python Schedule para orquestra√ß√£o de execu√ß√µes peri√≥dicas, com retry logic e exponential backoff para resili√™ncia. Os dados s√£o organizados no S3 em duas camadas: full-load (snapshot completo inicial) e cdc/ (arquivos incrementais com timestamp). Alcan√ßa 70% de redu√ß√£o no tamanho de armazenamento comparado a CSV tradicional, preparando dados otimizados para consumo downstream em arquiteturas Delta Lake e Lakehouse. Inclui logging estruturado, tratamento robusto de erros e suporte a m√∫ltiplas tabelas via configura√ß√£o JSON, demonstrando dom√≠nio em processamento incremental, otimiza√ß√£o de storage cloud-native e automa√ß√£o de pipelines ETL enterprise-grade.",
        technologies: ["Python", "Pandas", "AWS S3", "Parquet", "Kaggle API", "CDC", "boto3", "PyArrow"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT", "Cloud AWS", "DataOps"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_cdc_kaggle.png`,
        github: "https://github.com/tmarsbr/cdc-kaggle",
        demo: "",
        metrics: "1.5GB/m√™s processados, 24 exec./dia, 3-5min tempo m√©dio, 70% economia storage, 99.9% uptime",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    // =====================================================
    // PROJETO: Data Lake AWS (NOVO)
    // =====================================================
    {
        id: 105,
        title: "Arquitetura Data Lake Escal√°vel: Democratiza√ß√£o de Dados para Otimiza√ß√£o de Cobran√ßas",
        impactPhrase: "‚≠ê Destaque | Engenharia de Dados",
        description: "‚òÅÔ∏è Arquitetura completa de Data Lake na AWS com 4 camadas (RAW/Bronze/Silver/Gold), orquestra√ß√£o via Airflow e governan√ßa para democratizar dados e otimizar cobran√ßa de inadimplentes.",
        longDescription: "Implementa√ß√£o end-to-end de Data Lake na AWS para centralizar dados de m√∫ltiplas fontes e democratizar acesso para times de neg√≥cio e ci√™ncia de dados. A arquitetura em camadas garante qualidade progressiva dos dados, enquanto Apache Airflow orquestra todo o pipeline. Empresa precisava otimizar processo de cobran√ßa de clientes inadimplentes, mas dados estavam fragmentados em m√∫ltiplos sistemas (RDS, APIs, arquivos). Data Lake centraliza todas as fontes, aplica transforma√ß√µes e disponibiliza dados prontos para modelos preditivos de inadimpl√™ncia. Arquitetura em 4 camadas: RAW (dados brutos), Bronze (validados), Silver (transformados), Gold (anal√≠ticos). Implementa ingest√£o de m√∫ltiplas fontes heterog√™neas, processamento massivo com Apache Spark no EMR, governan√ßa com IAM e pol√≠ticas de ciclo de vida, otimiza√ß√£o de custos com compress√£o Parquet, monitoramento e alertas com CloudWatch.",
        technologies: ["AWS S3", "Amazon RDS", "Amazon EMR", "Apache Spark", "Amazon Athena", "CloudWatch", "IAM", "Apache Airflow", "Parquet"],
        category: "Engenharia de Dados",
        subcategories: ["Data Warehouse", "Python", "Airflow"],
        image: `${process.env.PUBLIC_URL}/projects/capa_data_lake_aws.png`,
        architectureDiagramImage: `${process.env.PUBLIC_URL}/images/arquitetura_data_lake_aws.png`,
        github: "https://github.com/tmarsbr/aws-data-lake-architecture",
        demo: "",
        metrics: "4 camadas, 6 servi√ßos AWS, m√∫ltiplas fontes, orquestra√ß√£o Airflow, governan√ßa IAM",
        featured: true,
        hidden: false,
        complexity: 5,
        date: "2024"
    }
];

/**
 * Configura√ß√£o da Se√ß√£o de Projetos
 * @description Textos e limites para a se√ß√£o de projetos na Home
 */
export const projectsConfig = {
    title: "Projetos em Destaque",
    description: "Projetos pr√°ticos que demonstram dom√≠nio dos fundamentos: ETL robusto, SQL avan√ßado e automa√ß√£o com ferramentas de mercado.",
    maxProjects: 4
};

/**
 * Configura√ß√£o da P√°gina de Projetos
 * @description Textos e storytelling da p√°gina de projetos
 */
export const projectsPageConfig = {
    title: "Engenharia de Dados Aplicada",
    subtitle: "Coleta ‚Ä¢ Transforma√ß√£o ‚Ä¢ Qualidade ‚Ä¢ Entrega",
    description: "Mais do que escrever c√≥digo, meu objetivo √© garantir que os dados fluam de forma segura e eficiente. Aqui apresento como utilizo padr√µes de mercado para resolver gargalos de performance e garantir a qualidade da informa√ß√£o que chega √† ponta final.",
    philosophy: "C√≥digo test√°vel, dados validados e pipelines confi√°veis. O objetivo √© entregar valor atrav√©s de dados √≠ntegros e processos automatizados."
};
